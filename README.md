# Awesome-Visual-Aided-NLP
A list of Visual Aided NLP, which tries to mine knowledge and information from visual domain data to help natural language understanding and generation.


## Papers


### Language Modeling

* [Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision](https://aclanthology.org/2020.emnlp-main.162) - Tan and Bansal, **EMNLP 2021**. [[code]](https://github.com/airsplay/vokenization)
* [VidLanKD: Improving Language Understanding via Video-Distilled Knowledge Transfer](https://proceedings.neurips.cc/paper/2021/hash/ccdf3864e2fa9089f9eca4fc7a48ea0a-Abstract.html) - Tang et al, **NIPS 2021**. [[code]](https://github.com/zinengtang/VidLanKD)
* [Imagination-Augmented Natural Language Understanding](https://aclanthology.org/2022.naacl-main.326/) - Lu et al, **NAACL 2022**. [[code]](https://github.com/YujieLu10/IACE-NLU)
* [Visually-Augmented Language Modeling](https://arxiv.org/abs/2205.10178) - Wang et al, **ICLR 2023**. [[code]](https://github.com/YujieLu10/IACE-NLU)
* [What Does Vision Supervision Bring to Language Models? A Case Study of CLIP](https://openreview.net/forum?id=SdBfRJE9SX-) - Anonymous, **ICLR 2023 Submission**.
* [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2302.00923) - Zhang et al, **arxiv preprint 2023**. [[code]](https://github.com/amazon-science/mm-cot)


### Representation Learning

* [Imagined Visual Representations as Multimodal Embeddings](https://ojs.aaai.org/index.php/AAAI/article/view/11155) - Collell et al, **AAAI 2017**.
* [Learning Visually Grounded Sentence Representations](https://aclanthology.org/N18-1038) - Kiela et al, **NAACL 2018**.
* [Incorporating Visual Semantics into Sentence Representations within a Grounded Space](https://aclanthology.org/D19-1064) - Bordes et al, **EMNLP 2019**.
* [Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case](https://aclanthology.org/2020.coling-main.64) - Lindstr√∂m et al, **COLING 2020**.
* [Accurate Word Representations with Universal Visual Guidance](http://arxiv.org/abs/2012.15086) - Zhang et al, **arxiv preprint 2020**.
* [Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training](https://aclanthology.org/2021.conll-1.12) - Shahmohammadi et al, **CoNLL 2021**.
* [Which Apple Keeps Which Doctor Away? Colorful Word Representations With Visual Oracles](https://ieeexplore.ieee.org/document/9627795) - Zhang et al, **TASLP 2021**. [[code]](https://github.com/cooelf/AppleLM)
* [Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations](https://aclanthology.org/2022.acl-long.217) - Zhang et al, **ACL 2022**.
* [MCSE: Multimodal Contrastive Learning of Sentence Embeddings](https://aclanthology.org/2022.naacl-main.436) - Zhang et al, **NAACL 2022**. [[code]](https://github.com/uds-lsv/MCSE)
* [Non-Linguistic Supervision for Contrastive Learning of Sentence Embeddings](http://arxiv.org/abs/2209.09433) - Jian et al, **NIPS 2022**. [[code]](https://github.com/yiren-jian/NonLing-CSE)


### Generation

* [Open Domain Dialogue Generation with Latent Images](https://ojs.aaai.org/index.php/AAAI/article/view/17675) - Yang et al, **AAAI 2021**.
* [Maria: A Visual Experience Powered Conversational Agent](https://aclanthology.org/2021.acl-long.435/) - Liang et al, **ACL 2021**. [[code]](https://github.com/jokieleung/Maria)
* [Text is NOT Enough: Integrating Visual Impressions into Open-domain Dialogue Generation](https://dl.acm.org/doi/10.1145/3474085.3475568) - Shen et al, **ACMMM 2021**.
* [An imagination-based automatic evaluation metric for natural language generation](https://arxiv.org/abs/2106.05970) - Zhu et al, **arxiv preprint 2021**.

### Visual Knowledge in Language

* [Probing Contextual Language Models for Common Ground with Visual Representations](https://aclanthology.org/2021.naacl-main.422) - Gabriel et al, **NAACL 2021**.
* [Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-Modal Knowledge Transfer](https://aclanthology.org/2022.acl-long.196) - Jin et al, **ACL 2022**.
* [Things not Written in Text: Exploring Spatial Commonsense from Visual Signals](https://aclanthology.org/2022.acl-long.168) - Liu et al, **ACL 2022**.
* [Visual Commonsense in Pretrained Unimodal and Multimodal Models](https://aclanthology.org/2022.naacl-main.390) - Zhang et al, **NAACL 2022**.


### Multimodal Machine Translation

* [Awesome-Multimodal-Machine-Translation](https://github.com/ZihengZZH/awesome-multimodal-machine-translation)

### Multimodal Named Entity Recognition

* [Multimodal Named Entity Recognition for Short Social Media Posts](https://aclanthology.org/N18-1078) - Moon et al, **NAACL 2018**.
* [Visual Attention Model for Name Tagging in Multimodal Social Media](https://aclanthology.org/P18-1185/) - Lu et al, **ACL 2018**.
* [Adaptive Co-attention Network for Named Entity Recognition in Tweets](https://ojs.aaai.org/index.php/AAAI/article/view/11962) - Zhang et al, **AAAI 2018**.
* [Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts](https://dl.acm.org/doi/abs/10.1145/3394171.3413650) - Wu et al, **ACMMM 2020**.
* [Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer](https://aclanthology.org/2020.acl-main.306/) - Yu et al, **ACL 2020**.
* [Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance](https://ojs.aaai.org/index.php/AAAI/article/view/17687) - Zhang et al, **AAAI 2021**.
* [RpBERT: A Text-image Relation Propagation-based BERT Model for Multimodal NER](https://ojs.aaai.org/index.php/AAAI/article/view/17633) - Sun et al, **AAAI 2021**.
* [ITA: Image-Text Alignments for Multi-Modal Named Entity Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/17633) - Wang et al, **NAACL 2022**.

### Other

* [Image-Mediated Learning for Zero-Shot Cross-Lingual Document Retrieval](https://aclanthology.org/D15-1070/) - Funaki et al, **EMNLP 2015**.
* [Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval With Generative Models](https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_Look_Imagine_and_CVPR_2018_paper.html) - Gu et al, **CVPR 2018**.
* [Visual Grounding in Video for Unsupervised Word Translation](https://openaccess.thecvf.com/content_CVPR_2020/html/Sigurdsson_Visual_Grounding_in_Video_for_Unsupervised_Word_Translation_CVPR_2020_paper.html) - Sigurdsson et al, **CVPR 2020**.
* [Does Vision-and-Language Pretraining Improve Lexical Grounding?](https://aclanthology.org/2021.findings-emnlp.370/) - Yun et al, **EMNLP 2021 (Findings)**.
* [Improving Personalized Explanation Generation through Visualization](https://aclanthology.org/2022.acl-long.20) - Geng et al, **ACL 2022**.
